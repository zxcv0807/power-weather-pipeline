name: Deploy to AWS EC2

on:
  push:
    branches: [ main ]
    paths:
      - 'collectors/weather/**'
      - 'spark/**'
      - 'dags/**'
      - 'docker-compose.yml'
      - 'Dockerfile.airflow'
      - 'requirements.txt'
      - '.github/workflows/deploy-aws.yml'

jobs:
  deploy:
    runs-on: [self-hosted, aws-ec2]
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Deploy AWS Pipeline
        run: |
          # AWS EC2의 기존 프로젝트 경로로 이동
          cd /home/ubuntu/power-weather-pipeline
          
          # 1. 최신 코드 받기
          git pull origin main
          
          # 2. Docker 재배포
          # (중요: Airflow 권한 문제 방지를 위해 down -> chown -> up 순서 권장)
          docker compose down
          sudo chown -R 50000:0 dags logs plugins
          docker compose up --build -d
          
          # 3. Python 가상환경 업데이트
          source venv/bin/activate
          pip install -r requirements.txt
          
          # 4. Weather Producer 재시작
          pkill -f weather_producer.py || true
          nohup python -u collectors/weather/weather_producer.py >> weather.log 2>&1 &
          
          # 5. Spark Streaming 재시작
          pkill -f streaming_to_lake.py || true
          nohup python -u spark/streaming_to_lake.py >> stream.log 2>&1 &
          
          echo "✅ AWS EC2 Deployment Completed!"